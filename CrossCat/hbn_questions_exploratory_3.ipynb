{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis with BayesDB\n",
    "\n",
    "Adapted by Jon Clucas of the [Child Mind Institute](https://www.childmind.org) from a notebook that was prepared for The DARPA PPAML PI meeting, July 2017 by [Feras Saad](http://fsaad.mit.edu) of the MIT Probabilistic Computing Project (Probcomp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Jupyter environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the `jupyter_probcomp.magics` library, which provides BayesDB hooks for data exploration, plotting, querying, and analysis through this Jupyter notebook environment. The second cell allows plots from matplotlib and javascript to be shown inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_probcomp.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%vizgpm inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB `.bdb` file on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the `%bayesdb` magic to create a `.bdb` file on disk named `hbnq.bdb`. This file will store all the data and models created in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f resources/hbnq_3.bdb\n",
    "%bayesdb resources/hbnq_3.bdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from resources.functions import contingency\n",
    "test = pd.read_csv('data/DSM_Test3.csv')\n",
    "check = test.copy()\n",
    "test['Anx'] = np.nan\n",
    "test['adhd'] = np.nan\n",
    "test['asd'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting data from a `.csv` file into a BayesDB table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hbnq dataset is stored in the csv file `data/Train_Set.csv`. Each column of the csv file is a variable, and each row is a record. We use the `CREATE TABLE` BQL query, with the pathname of the csv file, to convert the csv data into a database table named `hbnq_t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql DROP TABLE IF EXISTS \"hbnq_t\"\n",
    "%bql CREATE TABLE \"hbnq_t\" FROM 'data/DSM_Train3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all datasets have missing values, and special tokens such as `NaN` or `NA` indicating a particular cell is missing. In the HBNQ data, empty strings are used. To tell BayesDB to treat empty strings as SQL `NULL` we use the `.nullify` command, followed by the name of the table and the string `''` which represents missing data. Over 30,000 cells have been converted to `NULL`, illustrating that the data is quite sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .nullify hbnq_t ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running basic queries on the table using BQL and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the HBNQ dataset has been loaded into at table, and missing values converted to `NULL`, we can run standard SQL queries to explore the contents of the data. For example, we can select the first 5 records. Observe that each row in the table is a participant, and each column is a variable. Scroll through the names in the header of the table to get a sense of the variables in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"hbnq_t\" LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the total number of records (i.e. participants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT COUNT(*) FROM \"hbnq_t\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%bql DROP TABLE IF EXISTS hbnq_t_subsample\n",
    "%bql .subsample_columns --seed=8 --keep=EID --keep=adhd --keep=asd --keep=Anx --keep=Dx2 hbnq_t hbnq_t_subsample 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB population for the HBNQ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of a \"population\" is a central concept in BayesDB. For a standard database table, such as `hbnq_t`, each column is associated with a [data type](https://sqlite.org/datatype3.html), which in sqlite3 are `TEXT`, `REAL`, `INTEGER`, and `BLOB`. For a BayesDB population, each variable is associated with a _statistical data type_. These statistical types, such as `NOMINAL`, `NUMERICAL`, `MAGNITUDE`, and `COUNTS`, specify the set of values and default probability distributions used for building probabilistic models of the data in the population. In this tutorial, we will use the `NUMERICAL` and `NOMINAL` statistical data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `GUESS SCHEMA FOR <table>` command from the Metamodeling Language (MML) in BayesDB to guess the statistical data types of variables in the table. The guesses use heuristics based on the contents in the cells. The `num_distinct` column shows the number of unique values for that variable, and the `reason` column explains which heuristic was used to make the guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hbnq = %bql SELECT * FROM \"hbnq_t\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml GUESS SCHEMA FOR \"hbnq_t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `country` column has been identified as the `key`, since it is a unique identifer for each row. Many of the variables have been also correctly guessed  as `NUMERICAL`. A few variables have been incorrectly guessesd as `NOMINAL`, probably due to the sparsity in the dataset (the reason is that there are only a few distinct values for that variable in the table).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the HBNQ variables and their statistical datatypes, we use MML to create a population for the `hbnq_t_subsample` table. We will call the population `hbnq`. The population schema uses the statistical types guessed by BayesDB (from the previous cell) for all variables, except for a set of manual overrides for the incorrect guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonnumerical = ['EID', 'Sex', 'Dx', 'Anx', 'adhd', 'asd', 'train', 'Dx2']\n",
    "hbnq_nonnumerical = \", \".join(['\"{0}\"'.format(c.encode(\"utf-8\")) for c in hbnq.columns if c not in nonnumerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mml -i hbnq_nonnumerical\n",
    "CREATE POPULATION \"hbnq\" FOR \"hbnq_t\" WITH SCHEMA (\n",
    "    -- Use the guesses from the previous cell for all variables.\n",
    "    GUESS STATTYPES FOR (*);\n",
    "\n",
    "    -- Manually override incorrectly guessed statistical types.\n",
    "    MODEL\n",
    "        \"Age\", \"ASSQ_01\", \"ASSQ_02\", \"ASSQ_03\", \"ASSQ_04\", \"ASSQ_05\", \"ASSQ_06\", \"ASSQ_07\", \"ASSQ_08\", \"ASSQ_09\", \"ASSQ_10\", \"ASSQ_11\", \"ASSQ_12\", \"ASSQ_13\", \"ASSQ_14\", \"ASSQ_15\", \"ASSQ_16\", \"ASSQ_17\", \"ASSQ_18\", \"ASSQ_19\", \"ASSQ_20\", \"ASSQ_21\", \"ASSQ_22\", \"ASSQ_23\", \"ASSQ_24\", \"ASSQ_25\", \"ASSQ_26\", \"ASSQ_27\", \"ASSQ_Total\", \"SCARED_P_01\", \"SCARED_P_02\", \"SCARED_P_03\", \"SCARED_P_04\", \"SCARED_P_05\", \"SCARED_P_06\", \"SCARED_P_07\", \"SCARED_P_08\", \"SCARED_P_09\", \"SCARED_P_10\", \"SCARED_P_11\", \"SCARED_P_12\", \"SCARED_P_13\", \"SCARED_P_14\", \"SCARED_P_15\", \"SCARED_P_16\", \"SCARED_P_17\", \"SCARED_P_18\", \"SCARED_P_19\", \"SCARED_P_20\", \"SCARED_P_21\", \"SCARED_P_22\", \"SCARED_P_23\", \"SCARED_P_24\", \"SCARED_P_25\", \"SCARED_P_26\", \"SCARED_P_27\", \"SCARED_P_28\", \"SCARED_P_29\", \"SCARED_P_30\", \"SCARED_P_31\", \"SCARED_P_32\", \"SCARED_P_33\", \"SCARED_P_34\", \"SCARED_P_35\", \"SCARED_P_36\", \"SCARED_P_37\", \"SCARED_P_38\", \"SCARED_P_39\", \"SCARED_P_40\", \"SCARED_P_41\", \"SCARED_P_GD\", \"SCARED_P_PN\", \"SCARED_P_SC\", \"SCARED_P_SH\", \"SCARED_P_SP\", \"SCARED_P_Total\", \"SCQ_01\", \"SCQ_08\", \"SCQ_09\", \"SCQ_10\", \"SCQ_11\", \"SCQ_12\", \"SCQ_13\", \"SCQ_14\", \"SCQ_15\", \"SCQ_16\", \"SCQ_17\", \"SCQ_18\", \"SCQ_19\", \"SCQ_20\", \"SCQ_21\", \"SCQ_22\", \"SCQ_23\", \"SCQ_24\", \"SCQ_25\", \"SCQ_26\", \"SCQ_27\", \"SCQ_28\", \"SCQ_29\", \"SCQ_30\", \"SCQ_31\", \"SCQ_32\", \"SCQ_33\", \"SCQ_34\", \"SCQ_35\", \"SCQ_36\", \"SCQ_37\", \"SCQ_38\", \"SCQ_39\", \"SCQ_40\", \"SCQ_Total\", \"SWAN_01\", \"SWAN_02\", \"SWAN_03\", \"SWAN_04\", \"SWAN_05\", \"SWAN_06\", \"SWAN_07\", \"SWAN_08\", \"SWAN_09\", \"SWAN_10\", \"SWAN_11\", \"SWAN_12\", \"SWAN_13\", \"SWAN_14\", \"SWAN_15\", \"SWAN_16\", \"SWAN_17\", \"SWAN_18\", \"SWAN_IN\", \"SWAN_HY\", \"SWAN_Total\", \"CSC_01C\", \"CSC_02C\", \"CSC_02P\", \"CSC_03C\", \"CSC_04C\", \"CSC_05C\", \"CSC_06C\", \"CSC_07C\", \"CSC_08C\", \"CSC_09C\", \"CSC_10C\", \"CSC_11C\", \"CSC_12C\", \"CSC_13C\", \"CSC_14C\", \"CSC_15C\", \"CSC_16C\", \"CSC_17C\", \"CSC_18C\", \"CSC_19C\", \"CSC_20C\", \"CSC_21C\", \"CSC_22C\", \"CSC_23C\", \"CSC_24C\", \"CSC_25C\", \"CSC_26C\", \"CSC_27C\", \"CSC_28C\", \"CSC_29C\", \"CSC_30C\", \"CSC_31C\", \"CSC_32C\", \"CSC_33C\", \"CSC_34C\", \"CSC_35C\", \"CSC_36C\", \"CSC_37C\", \"CSC_38C\", \"CSC_39C\", \"CSC_40C\", \"CSC_41C\", \"CSC_42C\", \"CSC_43C\", \"CSC_44C\", \"CSC_45C\", \"CSC_46C\", \"CSC_47C\", \"CSC_48C\", \"CSC_49C\", \"CSC_50C\", \"CSC_51C\", \"CSC_52C\", \"CSC_53C\", \"CSC_54C\", \"CSC_55aC\", \"CSC_55bC\", \"CSC_55cC\", \"CSC_55dC\", \"CSC_55eC\", \"CSC_55fC\", \"CSC_55hC\", \"CSC_55iC\"\n",
    "    AS NUMERICAL;\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing joint distributions of data in the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with the statistical data types of variables in the population, we can now use the plotting features of BayesDB to produce scatter plots and heatmaps for the marginal and (pairwise) joint distributions of variables of interest. The `.interactive_pairplot` command requires a flag `--population=<pop>` for the population name, followed a BQL query. It generates pairplots of the data in all pairs of columns yielded by the query . Below, we have selected five variables to pairplot (recall that we have 100 variables in total). What features do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to create more pairplots of the data before moving on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bql\n",
    ".interactive_pairplot --population=hbnq \n",
    "SELECT\n",
    "    \"Dx2\",\n",
    "    \"Anx\",\n",
    "    \"adhd\",\n",
    "    \"asd\"\n",
    "FROM hbnq_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an analysis schema for the population using CrossCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the `gapminder` population, the next step is to analyze the data by building probabilistic models which explain the data generating process. Probabilistic data analyses in BayesDB are specified using an `ANALYSIS SCHEMA`. The default model discovery engine in BayesDB is Cross-Categorization [(Crosscat)](http://jmlr.org/papers/v17/11-392.html). CrossCat is a Bayesian factorial mixture model which learns a full joint distribution over all variables in the population, using a divide-and-conquer approach. We will explore CrossCat more in this notebook.\n",
    "\n",
    "For now we use MML to declare the an analysis schema named `hbnq_crosscat` for the `hbnq` population. Note that that we have left the schema in `crosscat()` empty, which will apply the built-in defaults. Specifying custom analysis schemas is the subject of another tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "%mml CREATE ANALYSIS SCHEMA \"hbnq_crosscat\" FOR \"hbnq\" WITH BASELINE crosscat();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying the `hbnq_crosscat` analysis schema, we now need to initialize `ANALYSES` for the schema. We can think of an `ANALYSIS SCHEMA` as specifying a hypothesis space of explanations for the data generating process for the population, and each `ANALYSIS` is a candidate hypothesis. We start by creating only 1 analysis, which is initialized __randomly__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml INITIALIZE 1 ANALYSIS FOR \"hbnq_crosscat\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a CrossCat hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, CrossCat learns the full joint distribution of all variables in the population using divide-and-conquer:\n",
    "\n",
    "- First, CrossCat partitions the variables into a set of _views_; all the variables in a particular view are modeled jointly, and two variables in different views are independent of one another.\n",
    "- Second, within each view, CrossCat clusters the rows using a non-parametric mixture model.\n",
    "\n",
    "The name Cross-Categorization is derived from this two-step process: first categorize the variables into views, and then categorize the rows into clusters within each view of variables. It is important to note that two different views A and B are likely to induce different clusterings of the rows.\n",
    "\n",
    "To get a sense of CrossCat's hypothesis space, we can render the hypothesis specified by a particular analysis using the `.render_crosscat [options] <analysis_schema> <analysis_identifier>` plotting command. The `--subsample=50` option says to only show a subsample of 50 rows in the rendering (even though `gampinder_crosscat` is modeling all countries in the `gapminder` population); `--rowlabels=country` specifies which column in the table to use to label the rows. Finally `gapminder_crosscat 0` means to render the first (and only) analysis in the `gapminder_crosscat` anlaysis schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50 --rowlabels=EID --xticklabelsize=small --yticklabelsize=xx-small --progress=True --width=64 \\\n",
    "    hbnq_crosscat 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To view a full-size image of the rendering, either double click the image, or right-click and select \"Open image in new tab.\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we emphasize that the CrossCat hypothesis shown above is __randomly__ initialized based on the two-step clustering process we have described. Each block of columns shows a view of dependent variables. The clusters within a view are demarcated using solid pink lines. Each row is a country from `gapminder`. The color of a cell shows the magnitude of the data (normalized between 0 and 1, where light indicates lower values and dark indices higher values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BQL to query CrossCat models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CrossCat rendering, each pair of variables is either in the same view (and therefore probably dependent), or in different views (and therefore independent). We can query the detected probable dependencies between all pairs of variables using the `DEPENDENCE PROBABILITY` estimator in BQL. The next query produces a heatmap of all pairs of dependencies. In the heatmap below, each row and column is a variable, and the color of a cell is a value between 0 and 1 (lighter is nearer to 0, and darker is nearer to 1) indicating the amount of evidence for a predictive relationship or dependency between these two variables. Since we have initialized only 1 CrossCat analysis, each cell is exactly either 0 (if those variables are in different views), or 1 (variables are in the same view). Confirm that the blocks shown in the heatmap match up with the blocks of variables from the rendering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap ESTIMATE DEPENDENCE PROBABILITY FROM PAIRWISE VARIABLES OF hbnq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Crosscat hypotheses using MML `ANALYZE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized a CrossCat hypothesis and investigated its state, it is time to improve our initial guess by exploring the hypothesis space to find hypotheses that better explain the data. In particular, our single CrossCat analysis has both spurious dependencies as well as independencies between variables which we would expect to be depedenct (study the heatmap and rendering, can you locate some of these pairs?).\n",
    "\n",
    "We can improve the CrossCat hypothesis by using the MML `ANALYZE` command, which takes the name of an analysis schema, an amount of iterations or seconds, and optional arguments. It then searches for improved hypotheses. Here, the `OPTIMIZED` keyword indicates to BayesDB to use a faster inference engine --- we generally recommend using this flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml ANALYZE \"hbnq_crosscat\" FOR 200 ITERATIONS WAIT (OPTIMIZED);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the new CrossCat after running 200 steps of analysis. Study the dependent variables. Can you identify a \"theme\" or \"category\" which summarizes each view? For example, the fourth view from the left has variables \"residential electricty consumption\", \"electricity generation per person\", and \"hydro production\", indicating that this may view might summarize energy variables. Which dependencies or independencies are still spurious?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50 --rowlabels=EIN --xticklabelsize=small --yticklabelsize=xx-small --progress=True --width=64 \\\n",
    "    hbnq_crosscat 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again visualize the probability there exists a dependence, between all pairs of variables, using BQL. How does this heatmap differ qualitatively from the dependence probability heatmap we plotted prior to running `ANALYZE`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap ESTIMATE DEPENDENCE PROBABILITY FROM PAIRWISE VARIABLES OF hbnq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in addition to learning a clustering of variables, CrossCat additionally learns a clustering of the rows within each view. These clusters are separated using pink lines in the CrossCat rendering. We can use the `SIMILARITY IN THE CONTEXT OF <variable>` query in BQL to study CrossCat's row partition in the view of `<variable>`.\n",
    "\n",
    "In the heatmap below, each row and column is a participant, and the value of a cell (between 0 and 1) indicates the probability that those two countries are relevant for formulating predictions about each other. Do these clusters of participants make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap --label0=rowid --label1=EID --table=hbnq_t \\\n",
    "    ESTIMATE SIMILARITY IN THE CONTEXT OF \"Dx2\" FROM PAIRWISE hbnq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing more CrossCat analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we used `INITIALIZE 1 ANALYSIS FOR gapminder_crosscat` to create a single analysis. As a result, all of our heatmaps (such as a variable dependencies and row similarities) had \"sharp\" values (either 1 or 0). Since CrossCat has a very large hypothesis space, we can significantly improve modeling by creating an ensemble of analyses, where each analysis searches the hypothesis space for hypotheses that fit the data well. All queries in BQL will then become weighted averages of the query results from each individual analysis in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%multiprocess on` magic activates multiprocessing BayesDB which allow us to initialize, analyze and run queries on analyses using multiple cores on the host machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%multiprocess on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following MML command ensures the `hbnq_crosscat` analysis schema will have a total of 32 analyses in the ensemble (recall that we already initialized 1 analysis, so the 31 new analyses will be added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml INITIALIZE 32 MODELS IF NOT EXISTS FOR \"hbnq_crosscat\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we run 1 minute of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml ANALYZE \"hbnq_crosscat\" FOR 150 ITERATIONS WAIT (OPTIMIZED);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use produce some renderings of the analyses (here we choose 5, 7 and 15). Where is there consensus among these three analyses? Where do they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50 --rowlabels=EID --xticklabelsize=small --yticklabelsize=x-small hbnq_crosscat 5\n",
    "%mml .render_crosscat \\\n",
    "    --subsample=50 --rowlabels=EID --xticklabelsize=small --yticklabelsize=x-small hbnq_crosscat 7\n",
    "%mml .render_crosscat \\\n",
    "    --subsample=50 --rowlabels=EID --xticklabelsize=small --yticklabelsize=x-small hbnq_crosscat 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare imputations to actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = %bql INFER \"Anx\" FROM \"hbnq\" MODELED BY \"hbnq_crosscat\"\n",
    "test['Anx'] = temp\n",
    "temp = %bql INFER \"adhd\" FROM \"hbnq\" MODELED BY \"hbnq_crosscat\"\n",
    "test[\"adhd\"] = temp\n",
    "temp = %bql INFER \"asd\" FROM \"hbnq\" MODELED BY \"hbnq_crosscat\"\n",
    "test[\"asd\"] = temp\n",
    "del(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency(test, check, \"Anx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency(test, check, \"adhd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency(test, check, \"asd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring probable dependencies between variables and comparing CrossCat dependence probability to linear (Pearson R) correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, all BQL queries are aggregated across the 32 analyses in the ensemble. We will create a table named `dependencies` which contains the pairwise `DEPENDENCE PROBABILITY` values between the Gapminder variables. The value of a cell (between 0 and 1) is the fraction of analyses in the ensemble where those two variables are detected to be probably dependent (i.e. they are in the same view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE dependencies AS\n",
    "ESTIMATE\n",
    "    DEPENDENCE PROBABILITY AS \"depprob\"\n",
    "FROM PAIRWISE VARIABLES OF hbnq;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `dependencies` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"dependencies\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again summarize the `dependencies` table using a heatmap. Study this dependence heatmap, and compare it to the heatmap produced when there was only 1 analysis. Which common-sense dependencies were missed by the single model, but identified by the ensemble as probably dependent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, depprob FROM dependencies;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare dependence probabilities from CrossCat to linear (Pearson r) correlation values, a very common technique for finding predictive relationships. We can compute the Pearson R (and its p-value) in BayesDB using the `CORRELATION` and `CORRELATION PVALUE` queries. The following cell creates a table named `correlations`, which contains the R and p-value for all pairs of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE \"correlations\" AS\n",
    "ESTIMATE\n",
    "    CORRELATION AS \"correlation\",\n",
    "    CORRELATION PVALUE AS \"pvalue\"\n",
    "FROM PAIRWISE VARIABLES OF \"hbnq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `correlations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"correlations\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Emphasis__: There is a signficiant difference between `DEPENDENCE PROBABILITY`, `CORRELATION`, and `CORRELATION PVALUE`. We outline these differences below, which will help us make comparisons between predictive relationships detected by CrossCat versus Pearson correlation.\n",
    "\n",
    "- `DEPENDENCE PROBABILITY`: Returns a value between [0,1] indicating the __probability there exists__ a predictive relationship (statistical dependence) between two variables.\n",
    "\n",
    "- `CORRELATION`: Returns a value between [0,1] indicating the __strength__ of the linear relationsip between two variables, where 0 means no linear correlation, and 1 means perfect linear correlation.\n",
    "\n",
    "- `CORRELATION PVALUE`: Returns a value between (0, 1) indicating the tail probability of the observed correlation value between two variables, under the null hypothesis that the two variables have zero correlation.\n",
    "\n",
    "Based on these distinctions, there is no immediate way to numerically compare `DEPENDENCE PROBABILITY` with `CORRELATION/CORRELATION PVALUE`. However, it is possible to compare the inferences about predictive relationships that each method gives rise to, which we do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first produce a heatmap of the raw correlation values. The following query shows the raw correlation values (between 0 and 1) for all pairs of variables where the p-value is less than 0.01 (note that we are not accounting for multiple-testing using e.g. Bonferroni correction). Pairs of variables where the p-value exceeds 0.01 (and thus the null hypothesis of independence cannot be rejected) are shown in gray. The sparsity of the data makes it difficult to draw inferences about many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, \"correlation\" FROM \"correlations\" WHERE \"pvalue\" < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the heatmap, and compare it to the heatmap from `DEPENDENCE PROBABILITY`. The patterns of dependence relationships differ significantly, how?\n",
    "\n",
    "We can use BQL to find variables which CrossCat believes are probably dependent, but correlation believes are independent (either the null hypothesis of independence cannot be rejected, or the correlation value is significant and near zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: probability dependent.\n",
    "    \"dependencies\".\"depprob\" > 0.85\n",
    "    AND (\n",
    "    -- Correlation: cannot reject null hypothesis of independence.\n",
    "    \"correlations\".\"pvalue\" > 0.05\n",
    "    OR (\n",
    "    -- Correlation: linear relationship is significant and near zero.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" < 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: high uncertainty about dependence probability.\n",
    "    \"dependencies\".\"depprob\" < 0.05\n",
    "    AND (\n",
    "    -- Correlation: statistically significant dependence.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" > 0.15)\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
